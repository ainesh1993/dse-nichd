{"cells":[{"cell_type":"markdown","metadata":{"id":"50f5049991824cc68f7205296a9c6c88"},"source":["<a class=\"anchor\" id=\"top\"></a>\n","# Modeling Notebook\n","**Authors: Ainesh Pandey, Demian Gass, Gabriel Gilling**\n","\n","In this notebook, we read in the prepped datasets and start modeling on our selected outcome variables.\n","\n","## Table of Contents\n","\n","[Step 1: Import Required Packages](#step-1) <br>\n","[Step 2: Read and Prepare Datasets](#step-2) <br>\n","[Step 3: Modeling](#step-3) <br>\n","[Step 4: Save Results for Analysis](#step-4) <br>"]},{"cell_type":"markdown","metadata":{"id":"7db401c63c7640cbb129a974a6f6401c"},"source":["<a class=\"anchor\" id=\"step-1\"></a>\n","\n","## Import Required Packages"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"6bc4cd7f-a30a-4863-b7c2-5e8d8cb07e9c"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","from scipy import stats\n","import pickle\n","\n","from imblearn.under_sampling import NearMiss\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import classification_report\n","\n","from sklearn.model_selection import GridSearchCV\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.ensemble import RandomForestClassifier\n","from lightgbm import LGBMClassifier\n","\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","import time\n","\n","from scripts.my_path import PATH"]},{"cell_type":"markdown","metadata":{"id":"041d495c385a466d8fca5ef5f386a432"},"source":["[Back to Top](#top)"]},{"cell_type":"markdown","metadata":{"id":"9c86b505ab2e4d39880129b08c2eca42"},"source":["<a class=\"anchor\" id=\"step-2\"></a>\n","\n","## Read and Prepare Datasets\n","\n","### Input Datasets\n","\n","**Deltas Dataframe**: These are the primary inputs to our model."]},{"cell_type":"code","execution_count":2,"metadata":{"id":"5b39779de56c4ddbaec895005de5929c"},"outputs":[{"name":"stdout","output_type":"stream","text":["Rows: 9289\n","Columns: 209\n"]},{"data":{"text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>PublicID</th>\n      <th>V1BA01_KG_delta_V2BA01_KG</th>\n      <th>V2BA01_KG_delta_V3BA01_KG</th>\n      <th>V1BA01_LB_delta_V2BA01_LB</th>\n      <th>V2BA01_LB_delta_V3BA01_LB</th>\n      <th>V2BA02b1_delta_V3BA02b1</th>\n      <th>V2BA02a1_delta_V3BA02a1</th>\n      <th>V2BA02b2_delta_V3BA02b2</th>\n      <th>V2BA02a2_delta_V3BA02a2</th>\n      <th>V1CA04_delta_V3CA04</th>\n      <th>...</th>\n      <th>U2BC03a_delta_U3BC03a</th>\n      <th>U2BA04_delta_U3BA04</th>\n      <th>U2BB04_delta_U3BB04</th>\n      <th>U2BB01_delta_U3BB01</th>\n      <th>U2BB03_delta_U3BB03</th>\n      <th>U2BA02_DY_delta_U3BA02_DY</th>\n      <th>U2BC01_delta_U3BC01</th>\n      <th>U2BA02_WK_delta_U3BA02_WK</th>\n      <th>U2BB02_delta_U3BB02</th>\n      <th>U2BC03c_delta_U3BC03c</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>00001U</td>\n      <td>0.522239</td>\n      <td>0.663086</td>\n      <td>0.512680</td>\n      <td>0.516508</td>\n      <td>0.339237</td>\n      <td>0.440188</td>\n      <td>0.514788</td>\n      <td>0.5745</td>\n      <td>0-3</td>\n      <td>...</td>\n      <td>S-S</td>\n      <td>0-0</td>\n      <td>0.580346</td>\n      <td>0-0</td>\n      <td>0-0</td>\n      <td>0.502593</td>\n      <td>0-0</td>\n      <td>0.453007</td>\n      <td>0.519531</td>\n      <td>S-S</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>00004O</td>\n      <td>0.522239</td>\n      <td>0.663086</td>\n      <td>0.472393</td>\n      <td>0.482693</td>\n      <td>0.336700</td>\n      <td>0.601595</td>\n      <td>0.514788</td>\n      <td>0.5745</td>\n      <td>3-3</td>\n      <td>...</td>\n      <td>S-S</td>\n      <td>2-2</td>\n      <td>0.580346</td>\n      <td>1-1</td>\n      <td>2-2</td>\n      <td>0.499938</td>\n      <td>2-2</td>\n      <td>0.585123</td>\n      <td>0.541750</td>\n      <td>S-S</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>00007I</td>\n      <td>0.522239</td>\n      <td>0.663086</td>\n      <td>0.510677</td>\n      <td>0.521473</td>\n      <td>0.395469</td>\n      <td>0.431082</td>\n      <td>0.514788</td>\n      <td>0.5745</td>\n      <td>3-3</td>\n      <td>...</td>\n      <td>S-S</td>\n      <td>2-2</td>\n      <td>0.580346</td>\n      <td>1-1</td>\n      <td>2-2</td>\n      <td>0.666791</td>\n      <td>2-2</td>\n      <td>0.418275</td>\n      <td>0.513992</td>\n      <td>S-S</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>00008G</td>\n      <td>0.522239</td>\n      <td>0.663086</td>\n      <td>0.563604</td>\n      <td>0.554183</td>\n      <td>0.331190</td>\n      <td>0.447863</td>\n      <td>0.514788</td>\n      <td>0.5745</td>\n      <td>3-2</td>\n      <td>...</td>\n      <td>S-S</td>\n      <td>2-2</td>\n      <td>0.580346</td>\n      <td>1-1</td>\n      <td>2-2</td>\n      <td>0.499814</td>\n      <td>2-2</td>\n      <td>0.519989</td>\n      <td>0.547669</td>\n      <td>S-S</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>00015J</td>\n      <td>0.522239</td>\n      <td>0.663086</td>\n      <td>0.530073</td>\n      <td>0.483132</td>\n      <td>0.421487</td>\n      <td>0.579686</td>\n      <td>0.514788</td>\n      <td>0.5745</td>\n      <td>1-1</td>\n      <td>...</td>\n      <td>S-S</td>\n      <td>2-2</td>\n      <td>0.580346</td>\n      <td>1-1</td>\n      <td>2-2</td>\n      <td>0.583240</td>\n      <td>2-2</td>\n      <td>0.316562</td>\n      <td>0.602954</td>\n      <td>S-S</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows Ã— 209 columns</p>\n</div>","text/plain":"  PublicID  V1BA01_KG_delta_V2BA01_KG  V2BA01_KG_delta_V3BA01_KG  \\\n0   00001U                   0.522239                   0.663086   \n1   00004O                   0.522239                   0.663086   \n2   00007I                   0.522239                   0.663086   \n3   00008G                   0.522239                   0.663086   \n4   00015J                   0.522239                   0.663086   \n\n   V1BA01_LB_delta_V2BA01_LB  V2BA01_LB_delta_V3BA01_LB  \\\n0                   0.512680                   0.516508   \n1                   0.472393                   0.482693   \n2                   0.510677                   0.521473   \n3                   0.563604                   0.554183   \n4                   0.530073                   0.483132   \n\n   V2BA02b1_delta_V3BA02b1  V2BA02a1_delta_V3BA02a1  V2BA02b2_delta_V3BA02b2  \\\n0                 0.339237                 0.440188                 0.514788   \n1                 0.336700                 0.601595                 0.514788   \n2                 0.395469                 0.431082                 0.514788   \n3                 0.331190                 0.447863                 0.514788   \n4                 0.421487                 0.579686                 0.514788   \n\n   V2BA02a2_delta_V3BA02a2 V1CA04_delta_V3CA04  ... U2BC03a_delta_U3BC03a  \\\n0                   0.5745                 0-3  ...                   S-S   \n1                   0.5745                 3-3  ...                   S-S   \n2                   0.5745                 3-3  ...                   S-S   \n3                   0.5745                 3-2  ...                   S-S   \n4                   0.5745                 1-1  ...                   S-S   \n\n  U2BA04_delta_U3BA04 U2BB04_delta_U3BB04 U2BB01_delta_U3BB01  \\\n0                 0-0            0.580346                 0-0   \n1                 2-2            0.580346                 1-1   \n2                 2-2            0.580346                 1-1   \n3                 2-2            0.580346                 1-1   \n4                 2-2            0.580346                 1-1   \n\n  U2BB03_delta_U3BB03 U2BA02_DY_delta_U3BA02_DY U2BC01_delta_U3BC01  \\\n0                 0-0                  0.502593                 0-0   \n1                 2-2                  0.499938                 2-2   \n2                 2-2                  0.666791                 2-2   \n3                 2-2                  0.499814                 2-2   \n4                 2-2                  0.583240                 2-2   \n\n  U2BA02_WK_delta_U3BA02_WK U2BB02_delta_U3BB02 U2BC03c_delta_U3BC03c  \n0                  0.453007            0.519531                   S-S  \n1                  0.585123            0.541750                   S-S  \n2                  0.418275            0.513992                   S-S  \n3                  0.519989            0.547669                   S-S  \n4                  0.316562            0.602954                   S-S  \n\n[5 rows x 209 columns]"},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["with open('../Data/df_deltas.pkl', 'rb') as f:\n","    df_deltas = pickle.load(f)\n","\n","# Normalize numeric features in deltas dataframe\n","for column in df_deltas.columns:\n","    if df_deltas[column].dtype == 'float64':\n","        df_deltas[column] = (df_deltas[column] - df_deltas[column].min()) / (df_deltas[column].max() - df_deltas[column].min())\n","\n","print('Rows:',    df_deltas.shape[0])\n","print('Columns:', df_deltas.shape[1])\n","df_deltas.head()"]},{"cell_type":"markdown","metadata":{"id":"c258045bdd004bddbde7507fe30a6544"},"source":["**Covariates Dataframe**: We will adjust our models for selected demographic and socio-economic variables."]},{"cell_type":"code","execution_count":3,"metadata":{"id":"486bd5d38a1b43c587d003f7ab72aff1"},"outputs":[{"name":"stdout","output_type":"stream","text":["Rows: 9289\n","Columns: 17\n"]},{"data":{"text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>GAwks_screen</th>\n      <th>Age_at_V1</th>\n      <th>eRace</th>\n      <th>BMI</th>\n      <th>Education</th>\n      <th>GravCat</th>\n      <th>SmokeCat1</th>\n      <th>SmokeCat2</th>\n      <th>Ins_Govt</th>\n      <th>Ins_Mil</th>\n      <th>Ins_Comm</th>\n      <th>Ins_Pers</th>\n      <th>Ins_Othr</th>\n      <th>V1AF14</th>\n      <th>V1AG01</th>\n      <th>V1AG11</th>\n      <th>PublicID</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.750</td>\n      <td>0.50000</td>\n      <td>7</td>\n      <td>0.265029</td>\n      <td>6.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>2.0</td>\n      <td>2.0</td>\n      <td>2.0</td>\n      <td>1.0</td>\n      <td>2.0</td>\n      <td>2.0</td>\n      <td>11</td>\n      <td>1</td>\n      <td>2</td>\n      <td>00001U</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.750</td>\n      <td>0.25000</td>\n      <td>6</td>\n      <td>0.180132</td>\n      <td>3.0</td>\n      <td>1.0</td>\n      <td>2.0</td>\n      <td>2.0</td>\n      <td>2.0</td>\n      <td>2.0</td>\n      <td>1.0</td>\n      <td>2.0</td>\n      <td>2.0</td>\n      <td>5</td>\n      <td>2</td>\n      <td>2</td>\n      <td>00004O</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.625</td>\n      <td>0.18750</td>\n      <td>5</td>\n      <td>0.147336</td>\n      <td>3.0</td>\n      <td>3.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>2.0</td>\n      <td>2.0</td>\n      <td>2.0</td>\n      <td>2.0</td>\n      <td>4</td>\n      <td>1</td>\n      <td>1</td>\n      <td>00007I</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.625</td>\n      <td>0.53125</td>\n      <td>5</td>\n      <td>0.239248</td>\n      <td>2.0</td>\n      <td>1.0</td>\n      <td>2.0</td>\n      <td>2.0</td>\n      <td>2.0</td>\n      <td>2.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>2.0</td>\n      <td>10</td>\n      <td>1</td>\n      <td>2</td>\n      <td>00008G</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.750</td>\n      <td>0.59375</td>\n      <td>5</td>\n      <td>0.114749</td>\n      <td>6.0</td>\n      <td>1.0</td>\n      <td>2.0</td>\n      <td>2.0</td>\n      <td>2.0</td>\n      <td>2.0</td>\n      <td>1.0</td>\n      <td>2.0</td>\n      <td>2.0</td>\n      <td>12</td>\n      <td>1</td>\n      <td>2</td>\n      <td>00015J</td>\n    </tr>\n  </tbody>\n</table>\n</div>","text/plain":"   GAwks_screen  Age_at_V1 eRace       BMI Education GravCat SmokeCat1  \\\n0         0.750    0.50000     7  0.265029       6.0     1.0       1.0   \n1         0.750    0.25000     6  0.180132       3.0     1.0       2.0   \n2         0.625    0.18750     5  0.147336       3.0     3.0       1.0   \n3         0.625    0.53125     5  0.239248       2.0     1.0       2.0   \n4         0.750    0.59375     5  0.114749       6.0     1.0       2.0   \n\n  SmokeCat2 Ins_Govt Ins_Mil Ins_Comm Ins_Pers Ins_Othr V1AF14 V1AG01 V1AG11  \\\n0       2.0      2.0     2.0      1.0      2.0      2.0     11      1      2   \n1       2.0      2.0     2.0      1.0      2.0      2.0      5      2      2   \n2       1.0      1.0     2.0      2.0      2.0      2.0      4      1      1   \n3       2.0      2.0     2.0      1.0      1.0      2.0     10      1      2   \n4       2.0      2.0     2.0      1.0      2.0      2.0     12      1      2   \n\n  PublicID  \n0   00001U  \n1   00004O  \n2   00007I  \n3   00008G  \n4   00015J  "},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["with open('../Data/df_covariates.pkl', 'rb') as f:\n","    df_covariates = pickle.load(f)\n","    \n","# Normalize numeric features in covariates dataframe\n","for column in df_covariates.columns:\n","    if df_covariates[column].dtype == 'float64':\n","        df_covariates[column] = (df_covariates[column] - df_covariates[column].min()) / (df_covariates[column].max() - df_covariates[column].min())\n","\n","print('Rows:',    df_covariates.shape[0])\n","print('Columns:', df_covariates.shape[1])\n","df_covariates.head()"]},{"cell_type":"markdown","metadata":{"id":"5b9aa052-54a4-4e51-bd00-a86171c0971e"},"source":["**Base Dataframe**: Combine the inputs (the deltas and the covariates) into the base dataset."]},{"cell_type":"code","execution_count":4,"metadata":{"id":"c2d60f1cb6584d619a5f0fdcaf311764"},"outputs":[{"data":{"text/plain":"(9289, 225)"},"metadata":{},"output_type":"display_data"},{"data":{"text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>PublicID</th>\n      <th>V1BA01_KG_delta_V2BA01_KG</th>\n      <th>V2BA01_KG_delta_V3BA01_KG</th>\n      <th>V1BA01_LB_delta_V2BA01_LB</th>\n      <th>V2BA01_LB_delta_V3BA01_LB</th>\n      <th>V2BA02b1_delta_V3BA02b1</th>\n      <th>V2BA02a1_delta_V3BA02a1</th>\n      <th>V2BA02b2_delta_V3BA02b2</th>\n      <th>V2BA02a2_delta_V3BA02a2</th>\n      <th>V1CA04_delta_V3CA04</th>\n      <th>...</th>\n      <th>SmokeCat1</th>\n      <th>SmokeCat2</th>\n      <th>Ins_Govt</th>\n      <th>Ins_Mil</th>\n      <th>Ins_Comm</th>\n      <th>Ins_Pers</th>\n      <th>Ins_Othr</th>\n      <th>V1AF14</th>\n      <th>V1AG01</th>\n      <th>V1AG11</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>00001U</td>\n      <td>0.522239</td>\n      <td>0.663086</td>\n      <td>0.512680</td>\n      <td>0.516508</td>\n      <td>0.339237</td>\n      <td>0.440188</td>\n      <td>0.514788</td>\n      <td>0.5745</td>\n      <td>0-3</td>\n      <td>...</td>\n      <td>1.0</td>\n      <td>2.0</td>\n      <td>2.0</td>\n      <td>2.0</td>\n      <td>1.0</td>\n      <td>2.0</td>\n      <td>2.0</td>\n      <td>11</td>\n      <td>1</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>00004O</td>\n      <td>0.522239</td>\n      <td>0.663086</td>\n      <td>0.472393</td>\n      <td>0.482693</td>\n      <td>0.336700</td>\n      <td>0.601595</td>\n      <td>0.514788</td>\n      <td>0.5745</td>\n      <td>3-3</td>\n      <td>...</td>\n      <td>2.0</td>\n      <td>2.0</td>\n      <td>2.0</td>\n      <td>2.0</td>\n      <td>1.0</td>\n      <td>2.0</td>\n      <td>2.0</td>\n      <td>5</td>\n      <td>2</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>00007I</td>\n      <td>0.522239</td>\n      <td>0.663086</td>\n      <td>0.510677</td>\n      <td>0.521473</td>\n      <td>0.395469</td>\n      <td>0.431082</td>\n      <td>0.514788</td>\n      <td>0.5745</td>\n      <td>3-3</td>\n      <td>...</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>2.0</td>\n      <td>2.0</td>\n      <td>2.0</td>\n      <td>2.0</td>\n      <td>4</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>00008G</td>\n      <td>0.522239</td>\n      <td>0.663086</td>\n      <td>0.563604</td>\n      <td>0.554183</td>\n      <td>0.331190</td>\n      <td>0.447863</td>\n      <td>0.514788</td>\n      <td>0.5745</td>\n      <td>3-2</td>\n      <td>...</td>\n      <td>2.0</td>\n      <td>2.0</td>\n      <td>2.0</td>\n      <td>2.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>2.0</td>\n      <td>10</td>\n      <td>1</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>00015J</td>\n      <td>0.522239</td>\n      <td>0.663086</td>\n      <td>0.530073</td>\n      <td>0.483132</td>\n      <td>0.421487</td>\n      <td>0.579686</td>\n      <td>0.514788</td>\n      <td>0.5745</td>\n      <td>1-1</td>\n      <td>...</td>\n      <td>2.0</td>\n      <td>2.0</td>\n      <td>2.0</td>\n      <td>2.0</td>\n      <td>1.0</td>\n      <td>2.0</td>\n      <td>2.0</td>\n      <td>12</td>\n      <td>1</td>\n      <td>2</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows Ã— 225 columns</p>\n</div>","text/plain":"  PublicID  V1BA01_KG_delta_V2BA01_KG  V2BA01_KG_delta_V3BA01_KG  \\\n0   00001U                   0.522239                   0.663086   \n1   00004O                   0.522239                   0.663086   \n2   00007I                   0.522239                   0.663086   \n3   00008G                   0.522239                   0.663086   \n4   00015J                   0.522239                   0.663086   \n\n   V1BA01_LB_delta_V2BA01_LB  V2BA01_LB_delta_V3BA01_LB  \\\n0                   0.512680                   0.516508   \n1                   0.472393                   0.482693   \n2                   0.510677                   0.521473   \n3                   0.563604                   0.554183   \n4                   0.530073                   0.483132   \n\n   V2BA02b1_delta_V3BA02b1  V2BA02a1_delta_V3BA02a1  V2BA02b2_delta_V3BA02b2  \\\n0                 0.339237                 0.440188                 0.514788   \n1                 0.336700                 0.601595                 0.514788   \n2                 0.395469                 0.431082                 0.514788   \n3                 0.331190                 0.447863                 0.514788   \n4                 0.421487                 0.579686                 0.514788   \n\n   V2BA02a2_delta_V3BA02a2 V1CA04_delta_V3CA04  ... SmokeCat1 SmokeCat2  \\\n0                   0.5745                 0-3  ...       1.0       2.0   \n1                   0.5745                 3-3  ...       2.0       2.0   \n2                   0.5745                 3-3  ...       1.0       1.0   \n3                   0.5745                 3-2  ...       2.0       2.0   \n4                   0.5745                 1-1  ...       2.0       2.0   \n\n  Ins_Govt Ins_Mil Ins_Comm Ins_Pers Ins_Othr V1AF14 V1AG01 V1AG11  \n0      2.0     2.0      1.0      2.0      2.0     11      1      2  \n1      2.0     2.0      1.0      2.0      2.0      5      2      2  \n2      1.0     2.0      2.0      2.0      2.0      4      1      1  \n3      2.0     2.0      1.0      1.0      2.0     10      1      2  \n4      2.0     2.0      1.0      2.0      2.0     12      1      2  \n\n[5 rows x 225 columns]"},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["df_base = df_deltas.merge(df_covariates, on='PublicID', how='inner')\n","\n","display(df_base.shape)\n","df_base.head()"]},{"cell_type":"markdown","metadata":{"id":"f512005037724e0c97d54d2789c06429"},"source":["### Targets Datasets\n","\n","**Target Dataframe**: The target variables we will be predicting on."]},{"cell_type":"code","execution_count":5,"metadata":{"id":"f15f473bd49a48248684658c9a510b21"},"outputs":[{"name":"stdout","output_type":"stream","text":["Rows: 9289\n","Columns: 19\n"]},{"data":{"text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>PEgHTN</th>\n      <th>ChronHTN</th>\n      <th>CMAD01a</th>\n      <th>CMAD01b</th>\n      <th>CMAD01c</th>\n      <th>CMAD01d</th>\n      <th>CMAD01e</th>\n      <th>CMAD01f</th>\n      <th>CMAD01g</th>\n      <th>CMAD01h</th>\n      <th>CMAE04a1c</th>\n      <th>CMAE04a2c</th>\n      <th>CMAE04a3c</th>\n      <th>CMAE04a4c</th>\n      <th>CMAE04a5c</th>\n      <th>Stillbirth</th>\n      <th>Miscarriage</th>\n      <th>Termination</th>\n      <th>PublicID</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>00001U</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>00004O</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>00007I</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>00008G</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>00015J</td>\n    </tr>\n  </tbody>\n</table>\n</div>","text/plain":"   PEgHTN  ChronHTN  CMAD01a  CMAD01b  CMAD01c  CMAD01d  CMAD01e  CMAD01f  \\\n0     NaN       NaN      NaN      NaN      NaN      NaN      NaN      NaN   \n1     0.0       0.0      NaN      NaN      NaN      NaN      NaN      NaN   \n2     0.0       0.0      NaN      NaN      NaN      NaN      NaN      NaN   \n3     0.0       0.0      NaN      NaN      NaN      NaN      NaN      NaN   \n4     0.0       0.0      NaN      NaN      NaN      NaN      NaN      NaN   \n\n   CMAD01g  CMAD01h  CMAE04a1c  CMAE04a2c  CMAE04a3c  CMAE04a4c  CMAE04a5c  \\\n0      NaN      NaN        NaN        NaN        NaN        NaN        NaN   \n1      NaN      NaN        0.0        0.0        0.0        0.0        0.0   \n2      NaN      NaN        0.0        0.0        0.0        0.0        0.0   \n3      NaN      NaN        0.0        0.0        0.0        0.0        0.0   \n4      NaN      NaN        0.0        0.0        0.0        0.0        0.0   \n\n   Stillbirth  Miscarriage  Termination PublicID  \n0         NaN          NaN          NaN   00001U  \n1         0.0          0.0          0.0   00004O  \n2         0.0          0.0          0.0   00007I  \n3         0.0          0.0          0.0   00008G  \n4         0.0          0.0          0.0   00015J  "},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["with open('../Data/df_targets.pkl', 'rb') as f:\n","    df_targets = pickle.load(f)\n","\n","print('Rows:',    df_targets.shape[0])\n","print('Columns:', df_targets.shape[1])\n","df_targets.head()"]},{"cell_type":"markdown","metadata":{"id":"7e72f318dfb94de482eff6e03f58d488"},"source":["### Auxiliary Datasets\n","\n","#### Variables Dictionary"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"a8c89a3e519b460e920bbb9e33193fe4"},"outputs":[{"data":{"text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Variable Name</th>\n      <th>Variable Label</th>\n      <th>Variable Type</th>\n      <th>Variable Code List</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>PublicID</td>\n      <td>Public nuMoM2b ID</td>\n      <td>Character</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>A02_Complete</td>\n      <td>(A02) Data entry status</td>\n      <td>Character</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>A02_Complete_1</td>\n      <td>(A02) Data entry status</td>\n      <td>Character</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>A02_Status</td>\n      <td>(A02) Validation status</td>\n      <td>Character</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>A02_Status_1</td>\n      <td>(A02) Validation status</td>\n      <td>Character</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n</div>","text/plain":"    Variable Name           Variable Label Variable Type Variable Code List\n0        PublicID        Public nuMoM2b ID     Character                NaN\n1    A02_Complete  (A02) Data entry status     Character                NaN\n2  A02_Complete_1  (A02) Data entry status     Character                NaN\n3      A02_Status  (A02) Validation status     Character                NaN\n4    A02_Status_1  (A02) Validation status     Character                NaN"},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["variables_df = pd.read_excel(PATH + '/nuMoM2b_Codebook_NICHD Data Challenge.xlsx',\n","                              sheet_name='nuMoM2b_Variables',\n","                              header=1,\n","                              usecols=['Variable Name', 'Variable Label', 'Variable Type', 'Variable Code List\\n(if Coded)'],\n","                              engine='openpyxl')\n","variables_df.columns = ['Variable Name', 'Variable Label', 'Variable Type', 'Variable Code List']\n","\n","variables_df.head()"]},{"cell_type":"markdown","metadata":{"id":"83c02c536a7246cb88e42b2dadb0c6e6"},"source":["### Results Datasets\n","\n","**Features Importance Dictionary**: We will store the feature importances for each target variable in this dictionary."]},{"cell_type":"code","execution_count":7,"metadata":{"id":"0516589954e14279882f47dd12dd857f"},"outputs":[],"source":["feature_results_dict = {}"]},{"cell_type":"markdown","metadata":{"id":"b657109324a44245826216dbefaa2e84"},"source":["**Model Results Dictionary**: We will store the model results for each target variables in this dictionary."]},{"cell_type":"code","execution_count":8,"metadata":{"id":"3e62ecc3acf34b6f9b4a569ed39e0c82"},"outputs":[],"source":["model_results_dict = {}"]},{"cell_type":"markdown","metadata":{"id":"61d1ca9b0a374ab38b520ef3d197f656"},"source":["[Back to Top](#top)"]},{"cell_type":"markdown","metadata":{"id":"0d9fcb42172c46d08f188a1889663fdf"},"source":["<a class=\"anchor\" id=\"step-3\"></a>\n","\n","## Modeling\n","\n","We will try the following modeling approaches, as explained in our README file.\n","- `Logistic Regression`\n","- `Random Forest`\n","- `Light Gradient-Boosted Model`"]},{"cell_type":"markdown","metadata":{"id":"9508afee-c3d2-4622-a2f5-fcaddd392896"},"source":["### Helper Functions\n","\n","These will be used in the master function."]},{"cell_type":"code","execution_count":9,"metadata":{"id":"99d7f97dcc1049ad8f9987147988269f"},"outputs":[],"source":["# The master function will use this helper function to prep the data for each target variable\n","def prep_and_split_data(target, df_base, df_targets):\n","    \n","    try:\n","        # append target feature to base dataframe\n","        df = df_base.merge(df_targets[['PublicID', target]], on='PublicID').drop('PublicID', axis=1)\n","\n","        # drop rows missing the output feature\n","        # print('  Num rows before dropping:', df.shape[0])\n","        # print('  Num missing values:', df[target].isna().sum())\n","        df = df.dropna(subset=[target])\n","        # print('  Num rows after dropping:', df.shape[0])\n","\n","        # split into X and y\n","        X = df.drop([target], axis = 1)\n","        y = df[target]\n","\n","        # drop correlated features\n","        # print('  Num columns before dropping correlated features:', X.shape[1])\n","        corr = X.corr()\n","        upper = corr.where(np.triu(np.ones(corr.shape), k=1).astype(np.bool))\n","        to_drop = [column for column in upper.columns if any(upper[column] > 0.85)]\n","        X.drop(to_drop, axis=1, inplace=True)\n","        # print('  Num columns after dropping correlated features:', X.shape[1])\n","\n","        # one-hot encode X\n","        X_dummied = pd.get_dummies(X, prefix_sep='__')\n","        # print('  Num columns after one-hot encoding:', X_dummied.shape[1])\n","        \n","        # train/test split with NearMiss undersampling\n","        X_dummied, y = NearMiss(version=3, n_neighbors_ver3=3).fit_resample(X_dummied, y)\n","        # print('  Num rows after Near-Miss undersampling:', X_dummied.shape[0])\n","        print('  Dataframe shape after cleaning:', str(X_dummied.shape))\n","\n","        # save dataframe for access during analysis phase\n","        filename = 'Target Pickles/df_' + target + '.pkl'\n","        with open(filename, 'wb') as f:\n","            pickle.dump(pd.concat([X_dummied, y], axis=1), f)\n","        # print('  Dataframe saved:', filename)\n","\n","        return train_test_split(X_dummied, y, test_size=0.3, random_state=42, stratify = y)\n","    \n","    except:\n","        pass"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"e5b667adfbdf478a8a05c918400e3626"},"outputs":[],"source":["# The master function will use this function to initialize the feature_results dataframe for each target variable\n","def initialize_feature_results(X_dummied):\n","    \n","    # initialize the feature results dataframe\n","    feature_results = pd.DataFrame(columns=['Feature', 'Variable1 Name', 'Variable1 Desc', 'Variable2 Name', 'Variable2 Desc',\n","                                            'LogR_FeatureImportance', 'RanF_FeatureImportance', 'LGBM_FeatureImportance'])\n","    \n","    # populate the \"Feature\" column\n","    feature_results['Feature'] = X_dummied.columns\n","\n","    # extract the individual features from the delta columns, nan for second feature if not a delta column\n","    feature_split = [x.split('__')[0].split('_delta_') for x in feature_results['Feature']]\n","    for x in feature_split:\n","        if(len(x) == 1): x.append(np.nan)\n","    feature_results['Variable1 Name'] = [x[0] for x in feature_split]\n","    feature_results['Variable2 Name'] = [x[1] for x in feature_split]\n","\n","    # extract the feature labels for all features\n","    feature_results['Variable1 Desc'] = feature_results[['Variable1 Name']].merge(variables_df[['Variable Name', 'Variable Label']], how='left',\n","                                                                                  left_on='Variable1 Name', right_on='Variable Name')['Variable Label']\n","    feature_results['Variable2 Desc'] = feature_results[['Variable2 Name']].merge(variables_df[['Variable Name', 'Variable Label']], how='left',\n","                                                                                  left_on='Variable2 Name', right_on='Variable Name')['Variable Label']\n","\n","    # extract the feature labels for all features\n","    feature_results['Variable1 Desc'] = feature_results[['Variable1 Name']].merge(variables_df[['Variable Name', 'Variable Label']], how='left',\n","                                                                                  left_on='Variable1 Name', right_on='Variable Name')['Variable Label']\n","    feature_results['Variable2 Desc'] = feature_results[['Variable2 Name']].merge(variables_df[['Variable Name', 'Variable Label']], how='left',\n","                                                                                  left_on='Variable2 Name', right_on='Variable Name')['Variable Label']\n","    \n","    return feature_results"]},{"cell_type":"markdown","metadata":{"id":"fa55cf83739049d6a26b5e8e92c2099c"},"source":["### Functions for Modeling Approaches\n","\n","***\n","**Logistic Regression**\n","***"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"01b5b5ae108e42c3a133f0d1fad76654"},"outputs":[],"source":["# create function that runs and tunes logistic regression, outputs results\n","def model_logisticregression_tuned(X_train, y_train, X_test, y_test):\n","    \n","    # create parameter grid to fine-tune model\n","    param_grid = { 'C': [0.0001, 0.001, 0.01, 0.1, 0.5, 1.0, 10.0] }\n","    \n","    # run the cross-validated grid search to identify the best parameters for the model\n","    CV_clf = GridSearchCV(estimator=LogisticRegression(penalty='l1', solver='liblinear', random_state=42),\n","                          scoring='f1', param_grid=param_grid, n_jobs=-1, verbose=0).fit(X_train, y_train)\n","    \n","    # extract the best parameters, as selected by the grid search\n","    best_params = CV_clf.best_params_\n","    best_C = best_params['C']\n","    \n","    # create the final RandomForestClassifier\n","    best_clf = LogisticRegression(random_state=42, C=best_C).fit(X_train, y_train)\n","    \n","    # predict on the test set\n","    y_pred = np.round(best_clf.predict(X_test))\n","    \n","    # Create dataframe for parameters and feature importances\n","    features_df = pd.DataFrame()\n","    features_df['Parameter'] = X_train.columns.to_list()\n","    features_df['Feature Importance'] = best_clf.coef_[0]\n","    \n","    # return the features dataframe and a classification report\n","    return [features_df, classification_report(y_test, y_pred, output_dict = True)]"]},{"cell_type":"markdown","metadata":{"id":"ffff7a71bf9946f98e9a512f82dd4b41"},"source":["***\n","**Random Forest**\n","***"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"e165cefe6a5144e3823392f3c8cd9dbc"},"outputs":[],"source":["# create function that runs and tunes random forest, outputs results\n","def model_randomforest_tuned(X_train, y_train, X_test, y_test):\n","    \n","    # create parameter grid to fine-tune model\n","    param_grid = { \n","        'n_estimators': range(100, 600, 100),\n","        'max_features': ['auto', 'log2', 0.2, 0.25, 0.33, 0.5],\n","        'max_depth' : [None, 4, 6, 8],\n","        'criterion' : ['gini', 'entropy']\n","    }\n","    \n","    # run the cross-validated grid search to identify the best parameters for the model\n","    CV_rfc = GridSearchCV(estimator=RandomForestClassifier(random_state=42), scoring='f1',\n","                          param_grid=param_grid, n_jobs=-1, verbose=0).fit(X_train, y_train)\n","    \n","    # extract the best parameters, as selected by the grid search\n","    best_params = CV_rfc.best_params_\n","    best_n_estimators = best_params['n_estimators']\n","    best_max_features = best_params['max_features']\n","    best_max_depth = best_params['max_depth']\n","    best_criterion = best_params['criterion']\n","    \n","    # create the final RandomForestClassifier\n","    best_rfc = RandomForestClassifier(random_state=42,\n","                                      max_features=best_max_features,\n","                                      n_estimators=best_n_estimators,\n","                                      max_depth=best_max_depth,\n","                                      criterion=best_criterion).fit(X_train, y_train)\n","    \n","    # predict on the test set\n","    y_pred = best_rfc.predict(X_test)\n","    \n","    # Create dataframe for parameters and feature importances\n","    features_df = pd.DataFrame()\n","    features_df['Parameter'] = X_train.columns.to_list()\n","    features_df['Feature Importance'] = best_rfc.feature_importances_\n","    \n","    # return the features dataframe and a classification report\n","    return [features_df, classification_report(y_test, y_pred, output_dict = True)]"]},{"cell_type":"markdown","metadata":{"id":"b6e22ae093ea462b8d612d0a114f725e"},"source":["***\n","**Light GBM**\n","***"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"0bc0c084806d4ef78956fc264b4410b5"},"outputs":[],"source":["# create function that runs lgbm, outputs results\n","def model_lgbm_tuned(X_train, y_train, X_test, y_test):\n","    \n","    # create parameter grid to fine-tune model\n","    param_grid = {\n","        'colsample_bytree': [0.8, 1.0],\n","        'max_depth': [15, 20, -1],\n","        'num_leaves': [10, 20, 31],\n","        'reg_alpha': [0, 0.5, 1.0],\n","        'reg_lambda': [0, 0.5, 1.0],\n","        'min_split_gain': [0, 0.2, 0.4],\n","        'subsample': [0.8, 1.0]\n","    }\n","    \n","    # run the cross-validated grid search to identify the best parameters for the model\n","    CV_lgb = GridSearchCV(estimator=LGBMClassifier(random_state=42), scoring='f1',\n","                          param_grid=param_grid, n_jobs=-1, verbose=0).fit(X_train, y_train)\n","    \n","    # extract the best parameters, as selected by the grid search\n","    best_params = CV_lgb.best_params_\n","    best_colsample_bytree = best_params['colsample_bytree']\n","    best_max_depth = best_params['max_depth']\n","    best_num_leaves = best_params['num_leaves']\n","    best_reg_alpha = best_params['reg_alpha']\n","    best_reg_lambda = best_params['reg_lambda']\n","    best_min_split_gain = best_params['min_split_gain']\n","    best_subsample = best_params['subsample']\n","    \n","    # create the final LGBMClassifier\n","    best_lgb = LGBMClassifier(random_state=42,\n","                              colsample_bytree=best_colsample_bytree,\n","                              max_depth=best_max_depth,\n","                              num_leaves=best_num_leaves,\n","                              reg_alpha=best_reg_alpha,\n","                              reg_lambda=best_reg_lambda,\n","                              min_split_gain=best_min_split_gain,\n","                              subsample=best_subsample).fit(X_train, y_train)\n","    \n","    # predict on the test set\n","    y_pred = best_lgb.predict(X_test)\n","\n","    # create dataframe for parameters and feature importances\n","    features_df = pd.DataFrame()\n","    features_df['Parameter'] = best_lgb.feature_name_\n","    features_df['Feature Importance'] = best_lgb.feature_importances_\n","\n","    # return the features dataframe and a classification report\n","    return [features_df, classification_report(y_test, y_pred, output_dict = True)]"]},{"cell_type":"markdown","metadata":{"id":"325fafcabd9e4a028fcfb5a300494c6b"},"source":["### Master Modeling Function"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"8a1259ec92ce41f383da58e7e41e0196"},"outputs":[],"source":["def run_models_tuned(target, df_base, df_targets):\n","    try:\n","        print('Modeling for target =', target)\n","\n","        # call the prep_and_split_data() helper function to extract the training and test sets\n","        X_train, X_test, y_train, y_test = prep_and_split_data(target, df_base, df_targets)\n","\n","        # initialize model results dictionary and feature results dataframe for selected target variable\n","        model_results = {}\n","        feature_results = initialize_feature_results(X_train)\n","\n","        # Run the logistic regression model, extract the results\n","        print('  Training and tuning Logistic Regression model...')\n","        t0 = time.time()\n","        logr_features, model_results['LogR'] = model_logisticregression_tuned(X_train, y_train, X_test, y_test)\n","        feature_results['LogR_FeatureImportance'] = feature_results[['Feature']].merge(logr_features, how='inner',\n","                                                                                       left_on='Feature', right_on='Parameter')['Feature Importance']\n","        t1 = time.time()\n","        print('    Done running Logistic Regression model in', str(int((t1-t0)/60)), 'mins and', str(int((t1-t0)%60)), 'secs')\n","\n","        # Run the random forest model, extract the results\n","        print('  Training and tuning Random Forest model...')\n","        t0 = time.time()\n","        ranf_features, model_results['RanF'] = model_randomforest_tuned(X_train, y_train, X_test, y_test)\n","        feature_results['RanF_FeatureImportance'] = feature_results[['Feature']].merge(ranf_features, how='inner',\n","                                                                                       left_on='Feature', right_on='Parameter')['Feature Importance']\n","        t1 = time.time()\n","        print('    Done running Random Forest model in', str(int((t1-t0)/60)), 'mins and', str(int((t1-t0)%60)), 'secs')\n","\n","        # Run the LGBM model, extract the results\n","        print('  Training and tuning LGBM model...')\n","        t0 = time.time()\n","        lgbm_features, model_results['LGBM'] = model_lgbm_tuned(X_train, y_train, X_test, y_test)\n","        feature_results['LGBM_FeatureImportance'] = feature_results[['Feature']].merge(lgbm_features, how='inner',\n","                                                                                       left_on='Feature', right_on='Parameter')['Feature Importance']\n","        t1 = time.time()\n","        print('    Done running LGBM model in', str(int((t1-t0)/60)), 'mins and', str(int((t1-t0)%60)), 'secs')\n","\n","        # Save results back to dictionaries\n","        model_results_dict[target]   = model_results\n","        feature_results_dict[target] = feature_results\n","        print('Modeling successful!')\n","        print()\n","        \n","    except:\n","        print('Modeling failed, due to error')\n","        print()\n","        pass"]},{"cell_type":"code","execution_count":17,"metadata":{"id":"be100a363e604b7aa9be779f0ebd5541","scrolled":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Modeling for target = CMAD01a\n","  Dataframe shape after cleaning: (168, 2407)\n","  Training and tuning Logistic Regression model...\n","    Done running Logistic Regression model in 0 mins and 1 secs\n","  Training and tuning Random Forest model...\n","    Done running Random Forest model in 1 mins and 58 secs\n","  Training and tuning LGBM model...\n","    Done running LGBM model in 6 mins and 23 secs\n","Modeling successful!\n","\n","Modeling for target = CMAD01b\n","  Dataframe shape after cleaning: (156, 2407)\n","  Training and tuning Logistic Regression model...\n","    Done running Logistic Regression model in 0 mins and 0 secs\n","  Training and tuning Random Forest model...\n","    Done running Random Forest model in 1 mins and 55 secs\n","  Training and tuning LGBM model...\n","    Done running LGBM model in 5 mins and 41 secs\n","Modeling successful!\n","\n","Modeling for target = CMAD01c\n","  Dataframe shape after cleaning: (162, 2407)\n","  Training and tuning Logistic Regression model...\n","    Done running Logistic Regression model in 0 mins and 0 secs\n","  Training and tuning Random Forest model...\n","    Done running Random Forest model in 2 mins and 2 secs\n","  Training and tuning LGBM model...\n","    Done running LGBM model in 5 mins and 53 secs\n","Modeling successful!\n","\n","Modeling for target = CMAD01d\n","  Dataframe shape after cleaning: (68, 2407)\n","  Training and tuning Logistic Regression model...\n","    Done running Logistic Regression model in 0 mins and 0 secs\n","  Training and tuning Random Forest model...\n","    Done running Random Forest model in 1 mins and 26 secs\n","  Training and tuning LGBM model...\n","    Done running LGBM model in 2 mins and 11 secs\n","Modeling successful!\n","\n","Modeling for target = CMAD01e\n","  Dataframe shape after cleaning: (36, 2407)\n","  Training and tuning Logistic Regression model...\n","    Done running Logistic Regression model in 0 mins and 0 secs\n","  Training and tuning Random Forest model...\n","    Done running Random Forest model in 1 mins and 15 secs\n","  Training and tuning LGBM model...\n","    Done running LGBM model in 2 mins and 11 secs\n","Modeling successful!\n","\n","Modeling for target = CMAD01f\n","  Dataframe shape after cleaning: (12, 2407)\n","  Training and tuning Logistic Regression model...\n","    Done running Logistic Regression model in 0 mins and 0 secs\n","  Training and tuning Random Forest model...\n","Modeling failed, due to error\n","\n","Modeling for target = CMAD01g\n","Modeling failed, due to error\n","\n","Modeling for target = CMAD01h\n","  Dataframe shape after cleaning: (42, 2407)\n","  Training and tuning Logistic Regression model...\n","    Done running Logistic Regression model in 0 mins and 0 secs\n","  Training and tuning Random Forest model...\n","    Done running Random Forest model in 1 mins and 18 secs\n","  Training and tuning LGBM model...\n","    Done running LGBM model in 2 mins and 11 secs\n","Modeling successful!\n","\n","Modeling for target = CMAE04a1c\n","  Dataframe shape after cleaning: (676, 3405)\n","  Training and tuning Logistic Regression model...\n","    Done running Logistic Regression model in 0 mins and 3 secs\n","  Training and tuning Random Forest model...\n","    Done running Random Forest model in 5 mins and 51 secs\n","  Training and tuning LGBM model...\n","    Done running LGBM model in 17 mins and 31 secs\n","Modeling successful!\n","\n","Modeling for target = CMAE04a2c\n","  Dataframe shape after cleaning: (482, 3405)\n","  Training and tuning Logistic Regression model...\n","    Done running Logistic Regression model in 0 mins and 2 secs\n","  Training and tuning Random Forest model...\n","    Done running Random Forest model in 4 mins and 29 secs\n","  Training and tuning LGBM model...\n","    Done running LGBM model in 14 mins and 37 secs\n","Modeling successful!\n","\n","Modeling for target = CMAE04a3c\n","  Dataframe shape after cleaning: (88, 3405)\n","  Training and tuning Logistic Regression model...\n","    Done running Logistic Regression model in 0 mins and 0 secs\n","  Training and tuning Random Forest model...\n","    Done running Random Forest model in 1 mins and 37 secs\n","  Training and tuning LGBM model...\n","    Done running LGBM model in 3 mins and 34 secs\n","Modeling successful!\n","\n","Modeling for target = CMAE04a4c\n","  Dataframe shape after cleaning: (20, 3405)\n","  Training and tuning Logistic Regression model...\n","    Done running Logistic Regression model in 0 mins and 0 secs\n","  Training and tuning Random Forest model...\n","    Done running Random Forest model in 1 mins and 15 secs\n","  Training and tuning LGBM model...\n","    Done running LGBM model in 2 mins and 23 secs\n","Modeling successful!\n","\n","Modeling for target = CMAE04a5c\n","Modeling failed, due to error\n","\n","Modeling for target = ChronHTN\n","  Dataframe shape after cleaning: (456, 3405)\n","  Training and tuning Logistic Regression model...\n","    Done running Logistic Regression model in 0 mins and 1 secs\n","  Training and tuning Random Forest model...\n","    Done running Random Forest model in 4 mins and 8 secs\n","  Training and tuning LGBM model...\n","    Done running LGBM model in 13 mins and 25 secs\n","Modeling successful!\n","\n","Modeling for target = Miscarriage\n","  Dataframe shape after cleaning: (164, 3408)\n","  Training and tuning Logistic Regression model...\n","    Done running Logistic Regression model in 0 mins and 0 secs\n","  Training and tuning Random Forest model...\n","    Done running Random Forest model in 1 mins and 51 secs\n","  Training and tuning LGBM model...\n","    Done running LGBM model in 5 mins and 33 secs\n","Modeling successful!\n","\n","Modeling for target = PEgHTN\n","  Dataframe shape after cleaning: (4080, 3405)\n","  Training and tuning Logistic Regression model...\n","    Done running Logistic Regression model in 0 mins and 33 secs\n","  Training and tuning Random Forest model...\n","    Done running Random Forest model in 48 mins and 25 secs\n","  Training and tuning LGBM model...\n","    Done running LGBM model in 112 mins and 14 secs\n","Modeling successful!\n","\n","Modeling for target = Stillbirth\n","  Dataframe shape after cleaning: (98, 3408)\n","  Training and tuning Logistic Regression model...\n","    Done running Logistic Regression model in 0 mins and 0 secs\n","  Training and tuning Random Forest model...\n","    Done running Random Forest model in 1 mins and 43 secs\n","  Training and tuning LGBM model...\n","    Done running LGBM model in 3 mins and 50 secs\n","Modeling successful!\n","\n","Modeling for target = Termination\n","  Dataframe shape after cleaning: (72, 3408)\n","  Training and tuning Logistic Regression model...\n","    Done running Logistic Regression model in 0 mins and 0 secs\n","  Training and tuning Random Forest model...\n","    Done running Random Forest model in 1 mins and 27 secs\n","  Training and tuning LGBM model...\n","    Done running LGBM model in 3 mins and 9 secs\n","Modeling successful!\n","\n"]}],"source":["for target in df_targets.columns.drop('PublicID').sort_values():\n","    run_models_tuned(target, df_base, df_targets)"]},{"cell_type":"markdown","metadata":{},"source":["<a class=\"anchor\" id=\"step-4\"></a>\n","\n","## Save Results for Analysis"]},{"cell_type":"code","execution_count":18,"metadata":{"id":"f48433f4181c44f68ab398a9367c90af"},"outputs":[],"source":["with open('../Results/model_results_dict.pkl', 'wb') as f:\n","    pickle.dump(model_results_dict, f)"]},{"cell_type":"code","execution_count":19,"metadata":{"id":"0a8960b76ab5471d9f5468c78b60ff2f"},"outputs":[],"source":["with open('../Results/feature_results_dict.pkl', 'wb') as f:\n","    pickle.dump(feature_results_dict, f)"]},{"cell_type":"markdown","metadata":{"id":"8bcf17ca6714407c92fb766702d8e633"},"source":["[Back to Top](#top)"]}],"metadata":{"kernelspec":{"display_name":"Python 3.8.2 64-bit","name":"python382jvsc74a57bd031f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.2"},"metadata":{"interpreter":{"hash":"31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"}}},"nbformat":4,"nbformat_minor":1}